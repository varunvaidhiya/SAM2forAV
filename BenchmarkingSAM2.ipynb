{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b0912b-f010-44d2-a9ce-c05394a09542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s.pt to 'yolov8s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.5M/21.5M [00:00<00:00, 32.3MB/s]\n",
      "frame loading (JPEG): 100%|██████████| 2395/2395 [02:09<00:00, 18.46it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.07 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 29.46 GiB is allocated by PyTorch, and 198.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Use the best YOLO model variant\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Initialize SAM 2 State\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m inference_state \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39minit_state(video_path\u001b[38;5;241m=\u001b[39moutput_frames_dir)\n\u001b[0;32m     71\u001b[0m predictor\u001b[38;5;241m.\u001b[39mreset_state(inference_state)\n\u001b[0;32m     73\u001b[0m ann_obj_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Initialize object ID\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam2\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam2\\Lib\\site-packages\\sam_2-1.0-py3.12-win-amd64.egg\\sam2\\sam2_video_predictor.py:49\u001b[0m, in \u001b[0;36mSAM2VideoPredictor.init_state\u001b[1;34m(self, video_path, offload_video_to_cpu, offload_state_to_cpu, async_loading_frames)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize an inference state.\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m compute_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# device of the model\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m images, video_height, video_width \u001b[38;5;241m=\u001b[39m load_video_frames(\n\u001b[0;32m     50\u001b[0m     video_path\u001b[38;5;241m=\u001b[39mvideo_path,\n\u001b[0;32m     51\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size,\n\u001b[0;32m     52\u001b[0m     offload_video_to_cpu\u001b[38;5;241m=\u001b[39moffload_video_to_cpu,\n\u001b[0;32m     53\u001b[0m     async_loading_frames\u001b[38;5;241m=\u001b[39masync_loading_frames,\n\u001b[0;32m     54\u001b[0m     compute_device\u001b[38;5;241m=\u001b[39mcompute_device,\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m inference_state \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     57\u001b[0m inference_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m images\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam2\\Lib\\site-packages\\sam_2-1.0-py3.12-win-amd64.egg\\sam2\\utils\\misc.py:230\u001b[0m, in \u001b[0;36mload_video_frames\u001b[1;34m(video_path, image_size, offload_video_to_cpu, img_mean, img_std, async_loading_frames, compute_device)\u001b[0m\n\u001b[0;32m    228\u001b[0m     images[n], video_height, video_width \u001b[38;5;241m=\u001b[39m _load_img_as_tensor(img_path, image_size)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m offload_video_to_cpu:\n\u001b[1;32m--> 230\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(compute_device)\n\u001b[0;32m    231\u001b[0m     img_mean \u001b[38;5;241m=\u001b[39m img_mean\u001b[38;5;241m.\u001b[39mto(compute_device)\n\u001b[0;32m    232\u001b[0m     img_std \u001b[38;5;241m=\u001b[39m img_std\u001b[38;5;241m.\u001b[39mto(compute_device)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.07 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 29.46 GiB is allocated by PyTorch, and 198.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO  # Assuming YOLOv8 is being used\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Paths and Directories\n",
    "video_path = './clip 2.mp4'\n",
    "output_frames_dir = './video_frames'\n",
    "output_video_path = 'segmented_video.mp4'\n",
    "\n",
    "# Create directory to store frames\n",
    "os.makedirs(output_frames_dir, exist_ok=True)\n",
    "\n",
    "# Load Video and Extract Frames\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "frame_names = []\n",
    "for i in range(frame_count):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_name = f'{i:05d}.jpg'\n",
    "    cv2.imwrite(os.path.join(output_frames_dir, frame_name), frame)\n",
    "    frame_names.append(frame_name)\n",
    "cap.release()\n",
    "\n",
    "\n",
    "# Custom Dataset for Batch Processing\n",
    "class VideoFrameDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, frame_names, frame_dir,target_size=(640, 640)):\n",
    "        self.frame_names = frame_names\n",
    "        self.frame_dir = frame_dir\n",
    "        self.target_size = target_size  # (Height, Width)\n",
    "    def __len__(self):\n",
    "        return len(self.frame_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = os.path.join(self.frame_dir, self.frame_names[idx])\n",
    "        frame = Image.open(frame_path).convert('RGB')\n",
    "        frame = frame.resize(self.target_size, Image.BILINEAR)\n",
    "        frame_np = np.array(frame)\n",
    "        # Convert to (C, H, W)\n",
    "        frame_np = frame_np.transpose(2, 0, 1)\n",
    "        frame_tensor = torch.from_numpy(frame_np).float() / 255.0  # Normalize to [0,1]\n",
    "        return frame_tensor, idx\n",
    "\n",
    "# Load Dataset\n",
    "dataset = VideoFrameDataset(frame_names, output_frames_dir)\n",
    "batch_size = 8 # Adjust batch size based on GPU memory\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load SAM 2 Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam2_checkpoint = \"./segment-anything-2/checkpoints/sam2_hiera_small.pt\"\n",
    "model_cfg = \"sam2_hiera_s.yaml\"\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "# Initialize YOLO model for object detection\n",
    "yolo_model = YOLO('yolov8s.pt').to(device)  # Use the best YOLO model variant\n",
    "\n",
    "# Initialize SAM 2 State\n",
    "inference_state = predictor.init_state(video_path=output_frames_dir)\n",
    "predictor.reset_state(inference_state)\n",
    "\n",
    "ann_obj_id = 1  # Initialize object ID\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Process frames in batches\n",
    "video_segments = {}\n",
    "for batch_frames, batch_indices in dataloader:\n",
    "    batch_frames = batch_frames.to(device)\n",
    "\n",
    "    # YOLO Object Detection with Gradient Checkpointing\n",
    "    with autocast():\n",
    "        results = yolo_model(batch_frames, imgsz=640)  # Ensure img size matches target_size\n",
    "\n",
    "    for i, frame_idx in enumerate(batch_indices):\n",
    "        detected_boxes = results[i].boxes.xyxy.cpu().numpy().astype(np.float32)  # Shape: (num_boxes, 4)\n",
    "\n",
    "        # Add detected boxes to SAM 2 for segmentation\n",
    "        for single_box in detected_boxes:\n",
    "            # SAM expects boxes in [x1, y1, x2, y2] format\n",
    "            _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                inference_state=inference_state,\n",
    "                frame_idx=int(frame_idx),\n",
    "                obj_id=ann_obj_id,\n",
    "                box=single_box.tolist(),  # Convert to list if necessary\n",
    "            )\n",
    "            ann_obj_id += 1\n",
    "\n",
    "        # Propagate masks in video after adding new objects\n",
    "        video_segments[int(frame_idx)] = {\n",
    "            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "            for i, out_obj_id in enumerate(out_obj_ids)\n",
    "        }\n",
    "          # Clear cache to free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save the Segmented Video\n",
    "# Assuming all frames are resized to target_size\n",
    "height, width = dataset.target_size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "for out_frame_idx in range(frame_count):\n",
    "    # Load the resized frame\n",
    "    frame_path = os.path.join(output_frames_dir, frame_names[out_frame_idx])\n",
    "    frame = cv2.imread(frame_path)\n",
    "    frame = cv2.resize(frame, (width, height))  # Ensure size matches target_size\n",
    "\n",
    "    if out_frame_idx in video_segments:\n",
    "        for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "            # Create a color overlay for the mask\n",
    "            color = np.array([0, 255, 0], dtype=np.uint8)  # Green mask\n",
    "            mask = (out_mask * 255).astype(np.uint8)\n",
    "            mask_rgb = cv2.merge([mask, mask, mask])\n",
    "            colored_mask = cv2.bitwise_and(color, color, mask=mask)\n",
    "            # Blend the mask with the frame\n",
    "            frame = cv2.addWeighted(frame, 1.0, colored_mask, 0.5, 0)\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n",
    "\n",
    "print(f'Segmented video saved to {output_video_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18904532-6253-46a9-abe1-4146e4d706c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|██████████| 153/153 [00:04<00:00, 34.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 476/476 [00:28<00:00, 16.78it/s]\n",
      "Processing batches:   0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 cars, 4.3ms\n",
      "1: 320x320 1 car, 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▏         | 1/77 [00:00<00:49,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 car, 7.4ms\n",
      "1: 320x320 1 car, 7.4ms\n",
      "Speed: 0.0ms preprocess, 7.4ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|▎         | 2/77 [00:01<00:39,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 cars, 6.5ms\n",
      "1: 320x320 2 cars, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|▍         | 3/77 [00:01<00:36,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 cars, 1 traffic light, 7.2ms\n",
      "1: 320x320 2 cars, 7.2ms\n",
      "Speed: 0.0ms preprocess, 7.2ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|▌         | 4/77 [00:02<00:35,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 3 cars, 1 traffic light, 5.5ms\n",
      "1: 320x320 3 cars, 5.5ms\n",
      "Speed: 0.0ms preprocess, 5.5ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|▋         | 5/77 [00:02<00:36,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 4 cars, 4.9ms\n",
      "1: 320x320 3 cars, 4.9ms\n",
      "Speed: 0.0ms preprocess, 4.9ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|▊         | 6/77 [00:03<00:36,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 3 cars, 4.8ms\n",
      "1: 320x320 2 cars, 4.8ms\n",
      "Speed: 0.0ms preprocess, 4.8ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%|▉         | 7/77 [00:03<00:36,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 1 car, 7.2ms\n",
      "1: 320x320 1 car, 7.2ms\n",
      "Speed: 0.0ms preprocess, 7.2ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%|█         | 8/77 [00:04<00:35,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 car, 5.2ms\n",
      "1: 320x320 1 person, 1 car, 5.2ms\n",
      "Speed: 0.0ms preprocess, 5.2ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%|█▏        | 9/77 [00:04<00:35,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 1 car, 5.5ms\n",
      "1: 320x320 2 persons, 1 car, 5.5ms\n",
      "Speed: 0.0ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|█▎        | 10/77 [00:05<00:35,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 persons, 1 car, 7.5ms\n",
      "1: 320x320 3 persons, 1 car, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|█▍        | 11/77 [00:05<00:36,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 3 persons, 1 car, 5.2ms\n",
      "1: 320x320 1 person, 1 car, 5.2ms\n",
      "Speed: 0.0ms preprocess, 5.2ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|█▌        | 12/77 [00:06<00:36,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 1 car, 5.5ms\n",
      "1: 320x320 1 car, 5.5ms\n",
      "Speed: 0.0ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|█▋        | 13/77 [00:06<00:35,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 3 persons, 3 cars, 5.0ms\n",
      "1: 320x320 3 persons, 2 cars, 5.0ms\n",
      "Speed: 0.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|█▊        | 14/77 [00:07<00:37,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 3 persons, 1 car, 7.5ms\n",
      "1: 320x320 3 persons, 3 cars, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|█▉        | 15/77 [00:08<00:38,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 3 persons, 3 cars, 4.0ms\n",
      "1: 320x320 3 persons, 1 car, 4.0ms\n",
      "Speed: 0.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|██        | 16/77 [00:09<00:39,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 3 persons, 1 car, 7.7ms\n",
      "1: 320x320 3 persons, 1 car, 7.7ms\n",
      "Speed: 0.0ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|██▏       | 17/77 [00:09<00:40,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 persons, 1 car, 4.5ms\n",
      "1: 320x320 2 persons, 1 car, 4.5ms\n",
      "Speed: 0.0ms preprocess, 4.5ms inference, 0.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|██▎       | 18/77 [00:10<00:40,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 persons, 2 cars, 6.0ms\n",
      "1: 320x320 2 persons, 2 cars, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|██▍       | 19/77 [00:11<00:40,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 persons, 2 cars, 7.0ms\n",
      "1: 320x320 1 person, 1 car, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|██▌       | 20/77 [00:11<00:40,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 1 car, 7.0ms\n",
      "1: 320x320 1 person, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|██▋       | 21/77 [00:12<00:39,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 5.0ms\n",
      "1: 320x320 1 person, 5.0ms\n",
      "Speed: 0.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|██▊       | 22/77 [00:13<00:37,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 4.5ms\n",
      "1: 320x320 1 person, 4.5ms\n",
      "Speed: 0.0ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|██▉       | 23/77 [00:13<00:36,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 7.0ms\n",
      "1: 320x320 4 persons, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|███       | 24/77 [00:14<00:37,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 3 persons, 4.0ms\n",
      "1: 320x320 2 persons, 1 car, 4.0ms\n",
      "Speed: 0.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|███▏      | 25/77 [00:15<00:37,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 1 car, 6.0ms\n",
      "1: 320x320 1 person, 2 cars, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|███▍      | 26/77 [00:16<00:39,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 persons, 2 cars, 8.5ms\n",
      "1: 320x320 2 persons, 4 cars, 1 fire hydrant, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|███▌      | 27/77 [00:19<01:11,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 3 persons, 4 cars, 1 fire hydrant, 4.5ms\n",
      "1: 320x320 2 persons, 2 cars, 4.5ms\n",
      "Speed: 0.0ms preprocess, 4.5ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|███▋      | 28/77 [00:23<01:48,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 2 persons, 2 cars, 4.0ms\n",
      "1: 320x320 1 person, 2 cars, 4.0ms\n",
      "Speed: 0.0ms preprocess, 4.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|███▊      | 29/77 [00:26<02:01,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 person, 2 cars, 7.0ms\n",
      "1: 320x320 2 cars, 7.0ms\n",
      "Speed: 0.0ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|███▉      | 30/77 [00:30<02:19,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 car, 6.0ms\n",
      "1: 320x320 1 car, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|████      | 31/77 [00:33<02:15,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 car, 4.5ms\n",
      "1: 320x320 1 car, 4.5ms\n",
      "Speed: 0.0ms preprocess, 4.5ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n",
      "Error processing frame 63 with box [     285.25       213.5      308.75         248]: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 8.00 GiB of which 2.10 GiB is free. Of the allocated memory 2.32 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|████▏     | 32/77 [00:36<02:10,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x320 1 car, 5.0ms\n",
      "1: 320x320 1 car, 5.0ms\n",
      "Speed: 0.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 320)\n",
      "Error processing frame 64 with box [     285.25       213.5      308.75         248]: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 8.00 GiB of which 2.10 GiB is free. Of the allocated memory 2.32 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Error processing frame 65 with box [        290       214.5         310         250]: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 8.00 GiB of which 2.10 GiB is free. Of the allocated memory 2.32 GiB is allocated by PyTorch, and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO  # Ensure you have ultralytics installed\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Paths and Directories\n",
    "video_path = './clip 1.mp4'\n",
    "output_frames_dir = './video_frames'\n",
    "output_video_path = 'segmented_video.mp4'\n",
    "\n",
    "# Create directory to store frames\n",
    "os.makedirs(output_frames_dir, exist_ok=True)\n",
    "\n",
    "# Load Video and Extract Frames\n",
    "def extract_frames(video_path, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    frame_names = []\n",
    "    for i in tqdm(range(frame_count), desc=\"Extracting frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_name = f'{i:05d}.jpg'\n",
    "        cv2.imwrite(os.path.join(output_frames_dir, frame_name), frame)\n",
    "        frame_names.append(frame_name)\n",
    "    cap.release()\n",
    "    return frame_names, fps\n",
    "\n",
    "frame_names, fps = extract_frames(video_path, output_frames_dir)\n",
    "\n",
    "# Custom Dataset for Batch Processing\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, frame_names, frame_dir, target_size=(320, 320)):\n",
    "        self.frame_names = frame_names\n",
    "        self.frame_dir = frame_dir\n",
    "        self.target_size = target_size  # (Width, Height)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = os.path.join(self.frame_dir, self.frame_names[idx])\n",
    "        frame = Image.open(frame_path).convert('RGB')\n",
    "        frame = frame.resize(self.target_size, Image.BILINEAR)\n",
    "        frame_np = np.array(frame)\n",
    "        # Convert to (C, H, W)\n",
    "        frame_np = frame_np.transpose(2, 0, 1)\n",
    "        frame_tensor = torch.from_numpy(frame_np).float() / 255.0  # Normalize to [0,1]\n",
    "        return frame_tensor, idx\n",
    "\n",
    "# Load Dataset and DataLoader\n",
    "dataset = VideoFrameDataset(frame_names, output_frames_dir)\n",
    "batch_size = 2  # Reduced batch size to minimize memory usage\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "# Check for CUDA and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load SAM 2 Model and move it to GPU if available\n",
    "sam2_checkpoint = \"./segment-anything-2/checkpoints/sam2_hiera_small.pt\"  # Ensure this path is correct\n",
    "model_cfg = \"sam2_hiera_s.yaml\"  # Ensure this config file is present\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "# Initialize YOLO model for object detection and move to GPU if available\n",
    "yolo_model = YOLO('yolov8s.pt').to(device)  # Using a smaller YOLO model variant\n",
    "\n",
    "# Initialize SAM 2 State with offloading frames to CPU to save GPU memory\n",
    "inference_state = predictor.init_state(\n",
    "    video_path=output_frames_dir,\n",
    "    offload_video_to_cpu=True  # Offload video frames to CPU\n",
    ")\n",
    "predictor.reset_state(inference_state)\n",
    "\n",
    "ann_obj_id = 1  # Initialize object ID\n",
    "\n",
    "# Dictionary to store segmentation masks\n",
    "video_segments = {}\n",
    "\n",
    "# Process frames in batches\n",
    "for batch_frames, batch_indices in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "    batch_frames = batch_frames.to(device, non_blocking=True)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations for inference\n",
    "        with autocast():  # Enable mixed precision\n",
    "            # Perform YOLO object detection on the batch\n",
    "            results = yolo_model(batch_frames, imgsz=320)  # Ensure img size matches target_size\n",
    "    \n",
    "    for i, frame_idx in enumerate(batch_indices):\n",
    "        detected_boxes = results[i].boxes.xyxy.cpu().numpy().astype(np.float32)  # Shape: (num_boxes, 4)\n",
    "        \n",
    "        # Add detected boxes to SAM 2 for segmentation\n",
    "        for single_box in detected_boxes:\n",
    "            # SAM expects boxes in [x1, y1, x2, y2] format\n",
    "            try:\n",
    "                _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                    inference_state=inference_state,\n",
    "                    frame_idx=int(frame_idx),\n",
    "                    obj_id=ann_obj_id,\n",
    "                    box=single_box.tolist(),  # Convert to list if necessary\n",
    "                )\n",
    "                ann_obj_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame_idx} with box {single_box}: {e}\")\n",
    "                continue\n",
    "    \n",
    "        # Propagate masks in video after adding new objects\n",
    "        try:\n",
    "            video_segments[int(frame_idx)] = {\n",
    "                out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "                for i, out_obj_id in enumerate(out_obj_ids)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error propagating masks for frame {frame_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Clear cache to free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save the Segmented Video\n",
    "def save_segmented_video(output_path, frame_names, video_segments, target_size=(320, 320)):\n",
    "    height, width = target_size\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for idx in tqdm(range(len(frame_names)), desc=\"Saving video\"):\n",
    "        frame_path = os.path.join(output_frames_dir, frame_names[idx])\n",
    "        frame = cv2.imread(frame_path)\n",
    "        frame = cv2.resize(frame, (width, height))  # Ensure size matches target_size\n",
    "    \n",
    "        if idx in video_segments:\n",
    "            for out_obj_id, out_mask in video_segments[idx].items():\n",
    "                # Create a color overlay for the mask\n",
    "                color = np.array([0, 255, 0], dtype=np.uint8)  # Green mask\n",
    "                mask = (out_mask * 255).astype(np.uint8)\n",
    "                mask_rgb = cv2.merge([mask, mask, mask])\n",
    "                colored_mask = cv2.bitwise_and(color, color, mask=mask)\n",
    "                # Blend the mask with the frame\n",
    "                frame = cv2.addWeighted(frame, 1.0, colored_mask, 0.5, 0)\n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()\n",
    "    print(f'Segmented video saved to {output_path}')\n",
    "\n",
    "save_segmented_video(output_video_path, frame_names, video_segments, target_size=(320, 320))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968a8ba5-089f-4eea-9248-50987bb639d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm  \u001b[38;5;66;03m# For progress bars\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Paths and Directories\u001b[39;00m\n\u001b[0;32m     13\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./clip 1.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sam2\\Lib\\site-packages\\torch\\cuda\\memory.py:170\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 170\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_emptyCache()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO  # Ensure you have ultralytics installed\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Paths and Directories\n",
    "video_path = './clip 1.mp4'\n",
    "output_frames_dir = './video_frames'\n",
    "output_video_path = 'segmented_video.mp4'\n",
    "\n",
    "# Create directory to store frames\n",
    "os.makedirs(output_frames_dir, exist_ok=True)\n",
    "\n",
    "# Load Video and Extract Frames\n",
    "def extract_frames(video_path, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    frame_names = []\n",
    "    for i in tqdm(range(frame_count), desc=\"Extracting frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_name = f'{i:05d}.jpg'\n",
    "        cv2.imwrite(os.path.join(output_frames_dir, frame_name), frame)\n",
    "        frame_names.append(frame_name)\n",
    "    cap.release()\n",
    "    return frame_names, fps\n",
    "\n",
    "frame_names, fps = extract_frames(video_path, output_frames_dir)\n",
    "\n",
    "# Custom Dataset for Batch Processing\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, frame_names, frame_dir, target_size=(320, 320)):\n",
    "        self.frame_names = frame_names\n",
    "        self.frame_dir = frame_dir\n",
    "        self.target_size = target_size  # (Width, Height)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = os.path.join(self.frame_dir, self.frame_names[idx])\n",
    "        frame = Image.open(frame_path).convert('RGB')\n",
    "        frame = frame.resize(self.target_size, Image.BILINEAR)\n",
    "        frame_np = np.array(frame)\n",
    "        # Convert to (C, H, W)\n",
    "        frame_np = frame_np.transpose(2, 0, 1)\n",
    "        frame_tensor = torch.from_numpy(frame_np).float() / 255.0  # Normalize to [0,1]\n",
    "        return frame_tensor, idx\n",
    "\n",
    "# Load Dataset and DataLoader\n",
    "dataset = VideoFrameDataset(frame_names, output_frames_dir)\n",
    "batch_size = 2  # Reduced batch size to minimize memory usage\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Check for CUDA and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load SAM 2 Model and move it to GPU if available\n",
    "sam2_checkpoint = \"./segment-anything-2/checkpoints/sam2_hiera_small.pt\"  # Ensure this path is correct\n",
    "model_cfg = \"sam2_hiera_s.yaml\"  # Ensure this config file is present\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "# Initialize YOLO model for object detection and move to GPU if available\n",
    "yolo_model = YOLO('yolov8s.pt').to(device)  # Using a smaller YOLO model variant\n",
    "\n",
    "# Initialize SAM 2 State with offloading frames to CPU to save GPU memory\n",
    "inference_state = predictor.init_state(\n",
    "    video_path=output_frames_dir,\n",
    "    offload_video_to_cpu=True  # Offload video frames to CPU\n",
    ")\n",
    "predictor.reset_state(inference_state)\n",
    "\n",
    "ann_obj_id = 1  # Initialize object ID\n",
    "\n",
    "# Dictionary to store segmentation masks\n",
    "video_segments = {}\n",
    "\n",
    "# Process frames in batches\n",
    "scaler = GradScaler()  # For mixed precision training\n",
    "for batch_frames, batch_indices in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "    batch_frames = batch_frames.to(device, non_blocking=True)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations for inference\n",
    "        with autocast():  # Enable mixed precision\n",
    "            # Perform YOLO object detection on the batch\n",
    "            results = yolo_model(batch_frames, imgsz=320)  # Ensure img size matches target_size\n",
    "    \n",
    "    for i, frame_idx in enumerate(batch_indices):\n",
    "        detected_boxes = results[i].boxes.xyxy.cpu().numpy().astype(np.float32)  # Shape: (num_boxes, 4)\n",
    "        \n",
    "        # Add detected boxes to SAM 2 for segmentation\n",
    "        for single_box in detected_boxes:\n",
    "            # SAM expects boxes in [x1, y1, x2, y2] format\n",
    "            try:\n",
    "                _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "                    inference_state=inference_state,\n",
    "                    frame_idx=int(frame_idx),\n",
    "                    obj_id=ann_obj_id,\n",
    "                    box=single_box.tolist(),  # Convert to list if necessary\n",
    "                )\n",
    "                ann_obj_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing frame {frame_idx} with box {single_box}: {e}\")\n",
    "                continue\n",
    "    \n",
    "        # Propagate masks in video after adding new objects\n",
    "        try:\n",
    "            video_segments[int(frame_idx)] = {\n",
    "                out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "                for i, out_obj_id in enumerate(out_obj_ids)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error propagating masks for frame {frame_idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Clear cache to free up memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save the Segmented Video\n",
    "def save_segmented_video(output_path, frame_names, video_segments, target_size=(320, 320)):\n",
    "    height, width = target_size\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    for idx in tqdm(range(len(frame_names)), desc=\"Saving video\"):\n",
    "        frame_path = os.path.join(output_frames_dir, frame_names[idx])\n",
    "        frame = cv2.imread(frame_path)\n",
    "        frame = cv2.resize(frame, (width, height))  # Ensure size matches target_size\n",
    "    \n",
    "        if idx in video_segments:\n",
    "            for out_obj_id, out_mask in video_segments[idx].items():\n",
    "                # Create a color overlay for the mask\n",
    "                color = np.array([0, 255, 0], dtype=np.uint8)  # Green mask\n",
    "                mask = (out_mask * 255).astype(np.uint8)\n",
    "                mask_rgb = cv2.merge([mask, mask, mask])\n",
    "                colored_mask = cv2.bitwise_and(color, color, mask=mask)\n",
    "                # Blend the mask with the frame\n",
    "                frame = cv2.addWeighted(frame, 1.0, colored_mask, 0.5, 0)\n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()\n",
    "    print(f'Segmented video saved to {output_path}')\n",
    "\n",
    "save_segmented_video(output_video_path, frame_names, video_segments, target_size=(320, 320))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90eacd-345d-4595-9ee8-718e382f8e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM2",
   "language": "python",
   "name": "sam2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
