{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc26bf-8788-4d22-99a0-1f6889e438e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "\n",
    "# Dataset class for Cityscapes with recursive file loading\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.image_files = self._load_files(self.image_dir)\n",
    "        self.mask_files = self._load_files(self.mask_dir)\n",
    "\n",
    "    def _load_files(self, dir_path):\n",
    "        all_files = []\n",
    "        for root, dirs, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".png\"):\n",
    "                    all_files.append(os.path.join(root, file))\n",
    "        return sorted(all_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        mask_path = self.mask_files[idx]\n",
    "\n",
    "        image = cv2.imread(img_path)[..., ::-1].copy()  # Convert BGR to RGB and fix strides issue\n",
    "        mask = cv2.imread(mask_path, 0).copy()  # Load mask in grayscale and fix strides issue\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define the transforms\n",
    "transform = Compose([ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "target_transform = ToTensor()\n",
    "\n",
    "# Define paths to Cityscapes validation dataset\n",
    "val_image_dir = \"segment-anything-2/cityscapes/leftImg8bit_trainvaltest/leftImg8bit/val\"\n",
    "val_mask_dir = \"segment-anything-2/cityscapes/gtFine_trainvaltest/gtFine/val\"\n",
    "\n",
    "# Create dataset and dataloader for validation\n",
    "val_dataset = CityscapesDataset(val_image_dir, val_mask_dir, transform=transform, target_transform=target_transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load SAM2 model and fine-tuned checkpoint\n",
    "sam2_checkpoint = 'sam2_cityscapes_optimized.pth'  # Fine-tuned pruned model checkpoint\n",
    "model_cfg = 'sam2_hiera_s.yaml'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Build and load model\n",
    "sam2_model = build_sam2(model_cfg, None, device=device)\n",
    "checkpoint = torch.load(sam2_checkpoint, map_location=device)\n",
    "sam2_model.load_state_dict(checkpoint, strict=False)  # Allow missing or unexpected keys\n",
    "sam2_model = sam2_model.to(device)\n",
    "sam2_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Initialize SAM2ImagePredictor\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# Metrics storage\n",
    "iou_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "latencies = []\n",
    "\n",
    "# For plotting\n",
    "plot_data = []\n",
    "\n",
    "# Define unnormalize transform\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (image, target) in enumerate(val_dataloader):\n",
    "        start_time = time.time()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Ensure the image is in (C, H, W) format by removing batch dimension if necessary\n",
    "        image_np = image.squeeze(0).cpu().numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
    "\n",
    "        # Predictor processing\n",
    "        predictor.set_image(image_np)  \n",
    "        \n",
    "        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(None, None, None)\n",
    "        \n",
    "        high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "\n",
    "        low_res_masks, _, _, _ = predictor.model.sam_mask_decoder(\n",
    "            image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "            image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            repeat_image=False,\n",
    "            multimask_output=False,\n",
    "            high_res_features=high_res_features,\n",
    "        )\n",
    "\n",
    "        pred_mask = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])[0]\n",
    "        \n",
    "        # Convert boolean mask to uint8 before resizing\n",
    "        pred_mask_uint8 = (torch.sigmoid(pred_mask).cpu().numpy() > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Resizing predicted mask to match target size using INTER_NEAREST for binary masks\n",
    "        pred_mask_resized = cv2.resize(pred_mask_uint8[0], (target.shape[-1], target.shape[-2]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Ensure target is binary (if not already)\n",
    "        target_binary = (target.cpu().numpy() > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Compute metrics with both arrays as binary values\n",
    "        iou = jaccard_score(target_binary.flatten(), pred_mask_resized.flatten(), average=\"macro\")\n",
    "        precision = precision_score(target_binary.flatten(), pred_mask_resized.flatten(), average=\"macro\", zero_division=0)\n",
    "        recall = recall_score(target_binary.flatten(), pred_mask_resized.flatten(), average=\"macro\", zero_division=0)\n",
    "        f1 = f1_score(target_binary.flatten(), pred_mask_resized.flatten(), average=\"macro\", zero_division=0)\n",
    "        latency = time.time() - start_time\n",
    "\n",
    "        # Store metrics\n",
    "        iou_scores.append(iou)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        latencies.append(latency)\n",
    "\n",
    "        print(f\"Batch {batch_idx}: IOU={iou:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}, Latency={latency:.4f}s\")\n",
    "\n",
    "        # Collect data for plotting for the first 4 images\n",
    "        if batch_idx < 4:\n",
    "            # Unnormalize image\n",
    "            image_unnorm = image * std + mean\n",
    "            image_np_plot = image_unnorm.squeeze(0).cpu().numpy().transpose(1, 2, 0)  # H x W x C\n",
    "            image_np_plot = np.clip(image_np_plot, 0, 1)  # Ensure values are between 0 and 1\n",
    "\n",
    "            # Get target mask\n",
    "            target_np = target.squeeze(0).squeeze(0).cpu().numpy()  # H x W\n",
    "\n",
    "            # Append to plot_data\n",
    "            plot_data.append((image_np_plot, pred_mask_resized, target_np))\n",
    "\n",
    "    # Compute overall metrics\n",
    "    mean_iou = np.mean(iou_scores)\n",
    "    mean_precision = np.mean(precisions)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    mean_latency = np.mean(latencies)\n",
    "\n",
    "    print(f\"Mean IOU: {mean_iou:.4f}\")\n",
    "    print(f\"Mean Precision: {mean_precision:.4f}\")\n",
    "    print(f\"Mean Recall: {mean_recall:.4f}\")\n",
    "    print(f\"Mean F1 Score: {mean_f1:.4f}\")\n",
    "    print(f\"Mean Latency (s): {mean_latency:.4f}\")\n",
    "\n",
    "    # Plot the actual images and predicted masks for any 4 images\n",
    "    for i, (image_np_plot, pred_mask_np, target_np) in enumerate(plot_data):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axes[0].imshow(image_np_plot)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Overlay the predicted mask onto the original image\n",
    "        axes[1].imshow(image_np_plot)\n",
    "        axes[1].imshow(pred_mask_np, cmap='jet', alpha=0.5)\n",
    "        axes[1].set_title('Predicted Mask Overlay')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # Overlay the ground truth mask onto the original image\n",
    "        axes[2].imshow(image_np_plot)\n",
    "        axes[2].imshow(target_np, cmap='jet', alpha=0.5)\n",
    "        axes[2].set_title('Ground Truth Mask Overlay')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
