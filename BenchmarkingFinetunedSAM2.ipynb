{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438c179-a87a-4c09-9c9a-9e30030e5544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the model configuration and checkpoint\n",
    "sam2_checkpoint = \"model_mvd.torch\"  # Path to the fine-tuned model weights\n",
    "model_cfg = \"sam2_hiera_l.yaml\"  # Path to the model configuration file\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")  # Load the fine-tuned model\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "predictor.model.eval()\n",
    "\n",
    "def inference_without_annotation(image_path):\n",
    "    # Load the image\n",
    "    Img = np.array(Image.open(image_path))[..., :3]  # Read the image (ignoring alpha channel)\n",
    "\n",
    "    # Resize the image for inference\n",
    "    r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]])  # Scaling factor\n",
    "    Img_resized = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "\n",
    "    # Prepare dummy points for the inference (since no annotations are provided)\n",
    "    points = [[[Img_resized.shape[1] // 2, Img_resized.shape[0] // 2]]]  # Center point as a simple prompt\n",
    "\n",
    "    # Perform inference\n",
    "    start_time = time.time()  # Start timer\n",
    "    predictor.set_image(Img_resized)  # Process the image with the model\n",
    "\n",
    "    # Prepare inputs for the prompt encoder\n",
    "    mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n",
    "        np.array(points), np.ones([len(points), 1]), box=None, mask_logits=None, normalize_coords=True)\n",
    "    \n",
    "    # Get embeddings from the prompt encoder\n",
    "    sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "        points=(unnorm_coords, labels), boxes=None, masks=None)\n",
    "    \n",
    "    # Decode masks using the SAM2 mask decoder\n",
    "    high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "    low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "        image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "        image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "        sparse_prompt_embeddings=sparse_embeddings,\n",
    "        dense_prompt_embeddings=dense_embeddings,\n",
    "        multimask_output=True,\n",
    "        repeat_image=unnorm_coords.shape[0] > 1,\n",
    "        high_res_features=high_res_features,\n",
    "    )\n",
    "    \n",
    "    # Post-process the masks\n",
    "    prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "    prd_masks = torch.sigmoid(prd_masks[:, 0])  # Convert logits to probabilities\n",
    "\n",
    "    # Measure the inference time\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    return prd_masks.cpu().numpy(), inference_time\n",
    "\n",
    "def visualize_masks(image, masks):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    for mask in masks:\n",
    "        plt.imshow(mask, alpha=0.5)  # Overlay each mask with some transparency\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def test_multiple_images(test_dir):\n",
    "    # Get a list of all test images\n",
    "    image_paths = [os.path.join(test_dir, img) for img in os.listdir(test_dir) if img.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    results = []\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Processing {image_path}...\")\n",
    "        predicted_masks, inference_time = inference_without_annotation(image_path)\n",
    "\n",
    "        # Load and resize the image to match the mask size\n",
    "        image = np.array(Image.open(image_path))[..., :3]\n",
    "        r = np.min([1024 / image.shape[1], 1024 / image.shape[0]])\n",
    "        image_resized = cv2.resize(image, (int(image.shape[1] * r), int(image.shape[0] * r)))\n",
    "\n",
    "        # Store results for later analysis or visualization\n",
    "        results.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"masks\": predicted_masks,\n",
    "            \"inference_time\": inference_time,\n",
    "            \"resized_image\": image_resized\n",
    "        })\n",
    "\n",
    "        # Visualize the results for each image\n",
    "        visualize_masks(image_resized, predicted_masks)\n",
    "        print(f\"Inference Time: {inference_time:.4f} seconds\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "test_dir = r\"segment-anything-2/Dataset/testing/images\"  # Replace with your test images directory\n",
    "\n",
    "# Perform testing on multiple images\n",
    "results = test_multiple_images(test_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
